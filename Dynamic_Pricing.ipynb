{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea56e65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T14:11:07.919662Z",
     "start_time": "2022-09-13T14:11:07.916030Z"
    }
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e15fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T10:51:01.521972Z",
     "start_time": "2023-06-28T10:51:00.223147Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=False)\n",
    "import time\n",
    "from numba import jit\n",
    "import pickle\n",
    "from bisect import bisect_left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e56f89",
   "metadata": {},
   "source": [
    "# Used functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c996e367-2232-4558-8c41-9bfb11f13cc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T10:51:05.693570Z",
     "start_time": "2023-06-28T10:51:05.688885Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# supplementary function\n",
    "@jit(nopython=True)\n",
    "def eta(p):\n",
    "    # define eta(p), here we use eta(p)=[1,p]\n",
    "    return np.array([1,p])\n",
    "\n",
    "def get_d_l(p, theta_S, m):\n",
    "    '''\n",
    "    This function is used to get d_l or d_u\n",
    "    '''\n",
    "    d_l = [np.dot(eta(p),theta_S[k-1]) for k in range(1,m+2)]\n",
    "    return d_l\n",
    "\n",
    "def get_y_l(d, H, sigma_m):\n",
    "    '''\n",
    "    This function is used to get y_l\n",
    "    '''\n",
    "    y = round(sum(d) + 4 * np.sqrt(H) * sigma_m)\n",
    "    return y\n",
    "        \n",
    "def get_V(T, V_TYPE):\n",
    "    if V_TYPE == \"LOG\":\n",
    "        return np.log2(T)\n",
    "    elif V_TYPE == '0.2':\n",
    "        return np.power(T, 0.2)\n",
    "    elif V_TYPE == '0.5':\n",
    "        return np.power(T, 0.5)\n",
    "    else:\n",
    "        raise ValueError(f\"Error: V_TYPE is not correct. Supported V_TYPEs are 'LOG', '0.2' and '0.5'.\")\n",
    "\n",
    "@jit(nopython=True)\n",
    "def get_T_2(T):\n",
    "    # epochs of solving DP\n",
    "    T_2=[2**i for i in range(1,round(np.log2(T))+1)] \n",
    "    return T_2\n",
    "\n",
    "def get_S(m, H, UNI_ALOC):\n",
    "    if UNI_ALOC:\n",
    "        # uniform allocation\n",
    "        S = uniform_allocation(m=m,H=H)\n",
    "    else:\n",
    "        S = list(range(1,m+2))\n",
    "        S.append(H+1)\n",
    "    return S\n",
    "\n",
    "def get_theta_S(a, b, m, S):\n",
    "    '''\n",
    "    theta = (a,b)\n",
    "    S is the re-pricing point list\n",
    "    m is the number of re-price points\n",
    "    '''\n",
    "    theta_S=[[sum((a[i-1] for i in range(S[k-1],S[k]))),sum((b[i-1] for i in range(S[k-1],S[k])))] for k in range(1,m+2)]\n",
    "    return theta_S\n",
    "    \n",
    "def get_sigma_S(sigma, m, S):\n",
    "    '''\n",
    "    sigma is the variance of random part demand\n",
    "    S is the re-pricing point list\n",
    "    m is the number of re-price points\n",
    "    '''\n",
    "    sigma_S=[np.sqrt(sum(sigma[i-1]**2 for i in range(S[k-1],S[k]))) for k in range(1,m+2)]\n",
    "    return sigma_S\n",
    "\n",
    "\n",
    "def get_noise_S(noise_all, m, T, S):\n",
    "    '''\n",
    "    noise_all is a 2-d dict\n",
    "    epoch(T) - re-pricing period(m)\n",
    "    '''\n",
    "    noise_S_all = [[sum(noise_all[t][i-1] for i in range(S[k-1],S[k])) for k in range(1,m+2)] for t in range(T)]\n",
    "    return noise_S_all\n",
    "\n",
    "\n",
    "def minus(x, y):\n",
    "    try:\n",
    "        len(x)\n",
    "        regret = [x[i]-y[i] for i in range(len(x))]\n",
    "    except TypeError:\n",
    "        regret = x - y\n",
    "    return regret\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def proj_idx(x, seq):    \n",
    "    '''\n",
    "    Return the location of x in seq (how many numbers in seq smaller than x)\n",
    "    '''\n",
    "    return np.searchsorted(seq, np.minimum(x,seq[-1]))\n",
    "\n",
    "@jit(nopython=True)\n",
    "def proj(x,seq):\n",
    "    '''\n",
    "    Return the biggest number in seq that smaller than x\n",
    "    '''\n",
    "    return seq[proj_idx(x, seq)] \n",
    "\n",
    "def mean_row(data):\n",
    "    mean_values = np.mean(data, axis=0)\n",
    "    mean_values.tolist()\n",
    "    return mean_values\n",
    "    \n",
    "@jit(nopython=True)\n",
    "def G(x):\n",
    "    '''\n",
    "    Holding and backlogging cost\n",
    "    '''\n",
    "    return holding * np.maximum(x, 0) + backlog * np.maximum(-x, 0)\n",
    "\n",
    "@jit(nopython=True)\n",
    "def uniform_allocation(m, H):\n",
    "    S = [1]\n",
    "    step = H / (m + 1)\n",
    "    for i in range(1, m + 1):\n",
    "        S.append(int(np.round(1 + i * step)))\n",
    "    S.append(H + 1)\n",
    "    return S\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7163d09b-633d-4cae-abd9-3255e1edc97f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_relative_profit_loss(rev_opt, rev_alg):\n",
    "    rel_loss = [(rev_opt[i] - rev_alg[i]) / rev_opt[i] for i in range(len(rev_alg))]\n",
    "    return rel_loss\n",
    "    \n",
    "\n",
    " def solve_DP_estimate(m, D, dist, y_grid):\n",
    "    '''\n",
    "    Realization of Eq(1)\n",
    "    \n",
    "    Input: \n",
    "        D: D empirical distribution (for every re-pricing period, every p)\n",
    "        dist: sample space of the m+1 noises distributions\n",
    "    Output:\n",
    "        rev_to_go: optimal revenue\n",
    "        p_policy: optimal price (for every level y, every re-pricing period)\n",
    "    '''\n",
    "    D_new = np.array([np.array([D[k][i] for i in range(p_number)]) for k in range(1,m+2)])\n",
    "    R = D_new * p_grid.reshape(1, -1)\n",
    "    \n",
    "    rev_to_go=np.empty((len(y_grid),m+1))\n",
    "    p_policy=np.empty((len(y_grid),m+1))\n",
    "    \n",
    "    for k in range(m+1,0,-1):\n",
    "        \n",
    "        y_index = proj_idx(y_grid.reshape(-1,1,1)-D_new[k-1].reshape(1,-1,1)-dist[k-1].reshape(1,1,-1),y_grid)\n",
    "        \n",
    "        if k == m+1:\n",
    "            # precompute proj(y-D[k][i]-noise_sample) and values of G()\n",
    "            G_val = G(y_grid[y_index])\n",
    "            temp = R[k-1].reshape(1,-1)-np.mean(G_val,axis=-1)\n",
    "            temp_max = np.max(temp,axis=-1)\n",
    "            \n",
    "        else:\n",
    "            # precompute proj(y-D[k][i]-noise_sample) and values of u_{k+1}()\n",
    "            u_val = temp_max[y_index]\n",
    "            temp = R[k-1].reshape(1,-1) + np.mean(u_val,axis=-1)\n",
    "            temp_max = np.max(temp,axis=-1)\n",
    "     \n",
    "        rev_to_go[:,k-1] = temp_max\n",
    "        p_policy[:,k-1]=p_grid[np.argmax(temp,axis=-1)]\n",
    "\n",
    "    rev_to_go_df=pd.DataFrame(rev_to_go,index=y_grid,columns=range(1,m+2))\n",
    "    p_policy_df=pd.DataFrame(p_policy,index=y_grid,columns=range(1,m+2))\n",
    "    \n",
    "    return rev_to_go_df, p_policy_df\n",
    "\n",
    "    \n",
    "# compute the revenue following a policy and a sequence of aggregated noise realization\n",
    "\n",
    "def compute_rev(m, theta_S, y_ini, policy, noise_seq, y_grid):\n",
    "    '''\n",
    "    Compute Revenue (in an epoch)\n",
    "\n",
    "    Input:\n",
    "        theta_S: parameter to combine demand (basis eta)\n",
    "        noise_seq: construct random demand part\n",
    "        \n",
    "        y_ini: order up to level\n",
    "        policy: price (for every level y, every re-pricing period)\n",
    "        y_grid: inventory grid\n",
    "        \n",
    "    Output:\n",
    "        rev_epoch: revnue for this epoch\n",
    "        p_used: the pricing we have used\n",
    "        d_list: the demand we have used\n",
    "    '''\n",
    "    rev_epoch = 0\n",
    "    p_used = []\n",
    "    d_list = []\n",
    "    y_level = y_ini # initialize the inventory level of the algorithm\n",
    "    \n",
    "    for k in range(1,m+2):\n",
    "        p_k = policy.loc[y_level,k]\n",
    "        p_used.append(p_k)\n",
    "        d_k = np.dot(eta(p_k),theta_S[k-1])+noise_seq[k-1]\n",
    "        d_list.append(d_k)\n",
    "\n",
    "        y_level = proj(y_level-d_k,y_grid) # update the inventory level\n",
    "        if k < m+1:\n",
    "            rev_epoch = rev_epoch+p_k*d_k\n",
    "        else:\n",
    "            rev_epoch = rev_epoch+p_k*d_k-G(y_level)\n",
    "        \n",
    "    return rev_epoch, p_used, d_list\n",
    "    \n",
    "\n",
    "# compute the optimal revenue under the full-information setting\n",
    "def solve_optimal_policy(m, sigma_S, theta_S, y_grid, monte_num=100): \n",
    "    '''\n",
    "    Compute Optimal Policy (in an epoch)\n",
    "\n",
    "    Input:\n",
    "        m: pricing changing points\n",
    "        sigma_S: used to generate random distribution of demand\n",
    "        theta_S: parameter to combine demand basis eta\n",
    "        noise_seq: construct random demand part\n",
    "        y_grid: inventory grid\n",
    "        \n",
    "    Output:\n",
    "        rev_epoch: revnue for this epoch\n",
    "        p_used: the pricing we have used\n",
    "        d_list: the demand we have used\n",
    "    '''\n",
    "    dist = [np.random.normal(0,sigma_S[k-1],monte_num) for k in range(1,m+2)]\n",
    "    D = {k:[np.dot(theta_S[k-1],eta(p)) for p in p_grid] for k in range(1,m+2)}\n",
    "    return solve_DP_estimate(m, D, dist, y_grid)\n",
    "    \n",
    "  \n",
    "\n",
    "def compute_oracle(T, m, S, theta_S, noise_inst, y_ini_optimal,pricing_optimal, y_grid):\n",
    "    '''\n",
    "    learning algorithm with rare updates for a given instance of noises\n",
    "    noise_inst is a list of T lists of m+1 random noises\n",
    "    '''    \n",
    "    rev_list_opt = []\n",
    "    for t in range(1,T+1):\n",
    "        # compute the revenue and prices of the optimal policy in epoch t\n",
    "        rev_epoch_opt, price_opt_t, d_opt_t = compute_rev(m, theta_S, y_ini_optimal, pricing_optimal, noise_inst[t-1], y_grid)\n",
    "        rev_list_opt.append(rev_epoch_opt)\n",
    "        \n",
    "    return rev_list_opt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9499e798-3161-4ff0-a9f1-a2d2b758d1c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T10:51:05.693570Z",
     "start_time": "2023-06-28T10:51:05.688885Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def learning_algorithm(m, T, H, S, theta_S, noise_inst, y_ini_optimal, pricing_optimal, y_u, y_grid, p_u, d_l_shift=0, sample_max=500, MODIFIED=False, CONST_b=True, V_TYPE=None, NO_OPT=False):\n",
    "    '''\n",
    "    learning algorithm with rare updates for a given instance of noises\n",
    "    noise_inst is a list of T lists of m+1 random noises\n",
    "    '''\n",
    "    # learning algorithm parameters\n",
    "    eta_lip = max(np.linalg.norm(eta(p_grid[i])-eta(p_grid[j]))/abs(p_grid[i]-p_grid[j]) for i in range(p_number) for j in range(p_number) if i != j)\n",
    "    L0 = max([np.sqrt(np.dot(theta_true[k],theta_true[k])) for k in range(H)])\n",
    "    L = max(np.sqrt(np.dot(eta(p_u),eta(p_u))),holding,backlog,eta_lip,L0)\n",
    "    T_2 = get_T_2(T)\n",
    "    sigma_S = get_sigma_S(sigma, m, S)\n",
    "    if CONST_b is False:\n",
    "        V = get_V(T, V_TYPE)  \n",
    "    # 1\n",
    "    gamma = [np.sqrt(S[k]-S[k-1])*(L+sigma_m*np.sqrt(2*np.log((m+1)*(H*T*L)**2))) for k in range(1,m+2)]\n",
    "    kapa = 2*L*max(gamma)*np.sqrt(2*dim*np.log(T*L**2+max(np.diff(S))))\n",
    "    \n",
    "    # lower bound of aggregated demand, used to compute the modified alg 2 (1)\n",
    "    if MODIFIED == 1:\n",
    "        d_l = np.array(get_d_l(p_u, theta_S, m)) - d_l_shift * np.array(sigma_S)\n",
    "        d_l.tolist()\n",
    "    else:\n",
    "        d_l = get_d_l(p_u, theta_S, m)\n",
    "    \n",
    "    price_hist={} # store price decision of the algorithm\n",
    "    d_hist={} # store the demand of the price decisions\n",
    "    Lambda={} # store the design matrix\n",
    "    theta={} # store estimated coefficients\n",
    "    Delta_hist={} # store the UCB for past prices\n",
    "    D={} # store estimated demand function\n",
    "    rev_list=[] # store revenue for each epoch of the algorithm\n",
    "    rev_list_opt=[] # store revenue for each epoch of the optimal offline policy\n",
    "    det=[] # store the determinant of the design matrix\n",
    "    y_alg=[] # store inventory decision of the algorithm\n",
    "    # update_period=[]\n",
    "    d_dtimesp=[] \n",
    "    \n",
    "    for t in range(1,T+1):\n",
    "        if CONST_b is False:\n",
    "            # b = b(t), changing with epoch t\n",
    "            b = [-(5+4*np.sin(V*np.pi*t/(2*T)))] * H\n",
    "            theta_S = get_theta_S(a, b, m, S)\n",
    "            \n",
    "        sample={} # store the samples of noises to be used in DP in each epoch \n",
    "        if (CONST_b is False) and (NO_OPT is False):\n",
    "            # compute the revenue and prices of the optimal policy in epoch t\n",
    "            rev_to_go_optimal, pricing_optimal = solve_optimal_policy(m, sigma_S, theta_S, y_grid, 500)    \n",
    "            # compute the optimal inventory level according  to u[1]\n",
    "            y_ini_optimal = y_grid[np.asarray(rev_to_go_optimal[1]).argmax()]\n",
    "            \n",
    "        if t==1: # 2. initialization\n",
    "            # update_period.append(t)\n",
    "            y_ini = y_u #order the largest quantity\n",
    "            y_alg.append(y_ini)\n",
    "            price_hist[t] = np.zeros(m+1)+p_l # pricing the lowest for m+1 re-pricing time\n",
    "            d_hist[t] = [np.dot(eta(price_hist[t][k-1]),theta_S[k-1]) + noise_inst[t-1][k-1] for k in range(1,m+2)]\n",
    "            rev_list.append(np.dot(price_hist[t],d_hist[t])-G(proj(y_ini-sum(d_hist[t]),y_grid)))\n",
    "            if NO_OPT is False:\n",
    "                rev_epoch_opt, price_opt_t, d_opt_t = compute_rev(m, theta_S, y_ini_optimal, pricing_optimal, noise_inst[t-1], y_grid)\n",
    "                # print(f'rev_epoch_opt={rev_epoch_opt}')\n",
    "                rev_list_opt.append(rev_epoch_opt)\n",
    "            \n",
    "            theta[t] = [[0]*dim]*(m+1) # set theta_1 to be zero vector\n",
    "            Lambda[t] = [(S[k]-S[k-1])*np.identity(dim) for k in range(1,m+2)] # set Lambda_1 to be identity matrix\n",
    "            Delta_hist[t] = [gamma[k-1]*np.sqrt(np.dot(eta(price_hist[t][k-1]),inv(Lambda[t][k-1]) @ eta(price_hist[t][k-1]))) for k in range(1,m+2)]\n",
    "            \n",
    "            det = [np.linalg.det(Lambda[t][k-1]) for k in range(1,m+2)]\n",
    "            \n",
    "            # update Lambda[t+1] and (\\sum d,\\sum d*p)\n",
    "            Lambda[t+1] = [Lambda[t][k-1]+np.outer(eta(price_hist[t][k-1]),eta(price_hist[t][k-1])) for k in range(1,m+2)] \n",
    "            \n",
    "            d_dtimesp = [np.array([d_hist[t][k-1],d_hist[t][k-1]*price_hist[t][k-1]]) for k in range(1,m+2)]\n",
    "            \n",
    "        # for t in T_2 or exists k s.t. det(lambda_k) is doubled, estimate the parameters and compute UCB\n",
    "        elif t in T_2 or sum(np.linalg.det(Lambda[t][k-1])>= 2*det[k-1] for k in range(1,m+2))>0: \n",
    "            # update_period.append(t)\n",
    "            # print(f'start solving DP in period {t}')\n",
    "            # update the detminant\n",
    "            # 5.\n",
    "            det=[np.linalg.det(Lambda[t][k-1]) for k in range(1,m+2)]\n",
    "            # estimated coefficient\n",
    "            theta[t]=[np.matmul(inv(Lambda[t][k-1]),d_dtimesp[k-1]) for k in range(1,m+2)]\n",
    "            \n",
    "            # for each k, compute Delta_tk(p) for p in grid, also for \\bar D_t^k\n",
    "            for k in range(1,m+2):\n",
    "                Delta = [gamma[k-1]*np.sqrt(np.dot(eta(p_grid[i]),inv(Lambda[t][k-1]) @ eta(p_grid[i]))) for i in range(p_number)]\n",
    "                UCB = [np.dot(theta[t][k-1],eta(p_grid[i])) for i in range(p_number)] # eta(p) times theta_t^k\n",
    "                # 6. different bar{D_t^k}(p)\n",
    "                if MODIFIED is False:\n",
    "                    D[k] = [min(UCB[i]+Delta[i], np.dot(theta_S[k-1],eta(p_l)), np.dot(theta_S[k-1],eta(p_u))+(S[k]-S[k-1])*(L**2)*(p_u-p_grid[i])) for i in range(p_number)]\n",
    "                elif MODIFIED == 1:\n",
    "                    D[k] = [min(UCB[i]+Delta[i], d_l[k-1]+(S[k]-S[k-1])*(L**2)*(p_u-p_grid[i])) for i in range(p_number)]\n",
    "                elif MODIFIED == 2:\n",
    "                    D[k] = [UCB[i]+Delta[i] for i in range(p_number)]\n",
    "\n",
    "                H_index = [s for s in range(1,t) if np.sqrt(t)*Delta_hist[s][k-1]<kapa]\n",
    "                # random choose sample_max number of histories\n",
    "                # Update: choose last sample_max number of histories \n",
    "                if len(H_index) > sample_max:\n",
    "                    # H_small=np.random.choice(H_index, sample_max, replace=False)\n",
    "                    H_small = H_index[-sample_max:]\n",
    "                else:\n",
    "                    H_small = H_index\n",
    "                # 7. hat{\\mu}_t^k \n",
    "                sample[k-1] = np.array([d_hist[s][k-1]-np.dot(theta[s][k-1],eta(price_hist[s][k-1])) for s in H_small])      \n",
    "            \n",
    "            # 8. solve the optimal inventory level for the estimated parameters and distributions\n",
    "            rev_to_go_estimate, p_policy_alg = solve_DP_estimate(m, D,sample,y_grid)\n",
    "            # 10.\n",
    "            y_ini = y_grid[np.asarray(rev_to_go_estimate[1]).argmax()]\n",
    "            y_alg.append(y_ini)\n",
    "            \n",
    "            # 9. compute the revenue and prices in epoch t\n",
    "            rev_epoch, price_hist[t],d_hist[t] = compute_rev(m, theta_S, y_ini,p_policy_alg,noise_inst[t-1], y_grid)\n",
    "            rev_list.append(rev_epoch)\n",
    "            # compute the revenue and prices of the optimal policy in epoch t\n",
    "            if NO_OPT is False:\n",
    "                rev_epoch_opt, price_opt_t, d_opt_t = compute_rev(m, theta_S, y_ini_optimal,pricing_optimal,noise_inst[t-1], y_grid)\n",
    "                # print(f'rev_epoch_opt={rev_epoch_opt}')\n",
    "                rev_list_opt.append(rev_epoch_opt)\n",
    "            \n",
    "            # update Lambda[t+1]\n",
    "            Lambda[t+1]=[Lambda[t][k-1]+np.outer(eta(price_hist[t][k-1]),eta(price_hist[t][k-1])) for k in range(1,m+2)] \n",
    "            d_dtimesp=[d_dtimesp[k-1]+np.array([d_hist[t][k-1],d_hist[t][k-1]*price_hist[t][k-1]]) for k in range(1,m+2)]\n",
    "            \n",
    "            # store the Delta of this epoch\n",
    "            Delta_hist[t]=[gamma[k-1]*np.sqrt(np.dot(eta(price_hist[t][k-1]),inv(Lambda[t][k-1]) @ eta(price_hist[t][k-1]))) for k in range(1,m+2)]\n",
    "            \n",
    "        else:\n",
    "            theta[t]=theta[t-1]\n",
    "            \n",
    "            # compute the revenue and prices in epoch t\n",
    "            rev_epoch,price_hist[t],d_hist[t]=compute_rev(m, theta_S, y_ini,p_policy_alg,noise_inst[t-1], y_grid)\n",
    "            # print(f'rev_epoch={rev_epoch}')\n",
    "            rev_list.append(rev_epoch)\n",
    "            y_alg.append(y_ini)\n",
    "\n",
    "            if NO_OPT is False:\n",
    "            # compute the revenue and prices of the optimal policy in epoch t\n",
    "                rev_epoch_opt,price_opt_t,d_opt_t=compute_rev(m, theta_S, y_ini_optimal,pricing_optimal,noise_inst[t-1], y_grid)\n",
    "                rev_list_opt.append(rev_epoch_opt)\n",
    "            \n",
    "            # update Lambda[t+1]\n",
    "            Lambda[t+1]=[Lambda[t][k-1]+np.outer(eta(price_hist[t][k-1]),eta(price_hist[t][k-1])) for k in range(1,m+2)] \n",
    "            d_dtimesp=[d_dtimesp[k-1]+np.array([d_hist[t][k-1],d_hist[t][k-1]*price_hist[t][k-1]]) for k in range(1,m+2)]\n",
    "            \n",
    "            # store the Delta of this epoch\n",
    "            Delta_hist[t]=[gamma[k-1]*np.sqrt(np.dot(eta(price_hist[t][k-1]),inv(Lambda[t][k-1]) @ eta(price_hist[t][k-1]))) for k in range(1,m+2)]\n",
    "\n",
    "    return rev_list, rev_list_opt, y_alg, theta    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e354bca-55dd-49d9-8be6-262e0410da8e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def Exp_Exp_algorithm(m, T, H, S, theta_S, noise_inst, y_grid, y_ini_optimal, pricing_optimal, d_u, sigma_S, sample_max=500, FRAC=1, CONST_b=True, V_TYPE=None, NO_OPT=False):    \n",
    "    '''\n",
    "    Step 1\n",
    "    Explore good policy\n",
    "    '''\n",
    "    # learning algorithm parameters\n",
    "    eta_lip = max(np.linalg.norm(eta(p_grid[i])-eta(p_grid[j]))/abs(p_grid[i]-p_grid[j]) for i in range(p_number) for j in range(p_number) if i != j)\n",
    "    L0 = max([np.sqrt(np.dot(theta_true[k],theta_true[k])) for k in range(H)])\n",
    "    L = max(np.sqrt(np.dot(eta(p_u),eta(p_u))), holding, backlog, eta_lip, L0)\n",
    "\n",
    "    if CONST_b is False:\n",
    "        V = get_V(T, V_TYPE)\n",
    "        sigma_S = get_sigma_S(sigma, m, S)\n",
    "            \n",
    "    # 1\n",
    "    gamma = [np.sqrt(S[k]-S[k-1])*(L+sigma_m*np.sqrt(2*np.log((m+1)*(H*T*L)**2))) for k in range(1,m+2)]\n",
    "    kapa = 2*L*max(gamma)*np.sqrt(2*dim*np.log(T*L**2+max(np.diff(S))))\n",
    "    \n",
    "    # upper bound of y level according to upper bound of aggregated demand\n",
    "    d_u = [d_u[i] + 3 * sigma_S[i] for i in range(len(sigma_S))]\n",
    "    y_0 = sum(d_u)\n",
    "    \n",
    "    price_hist={} # store price decision of the algorithm\n",
    "    d_hist={} # store the demand of the price decisions\n",
    "    Lambda={} # store the design matrix\n",
    "    theta={} # store estimated coefficients\n",
    "    Delta_hist={} # store the UCB for past prices\n",
    "    D={} # store estimated demand function\n",
    "    rev_list=[] # store revenue for each epoch of the algorithm\n",
    "    rev_list_opt=[] # store revenue for each epoch of the optimal offline policy\n",
    "    det=[] # store the determinant of the design matrix\n",
    "    # update_period=[]\n",
    "    d_dtimesp=[] \n",
    "    if FRAC == 1:\n",
    "        T0 = int(np.sqrt(T))\n",
    "    elif FRAC == 2:\n",
    "        T0 = int(pow(T,2/3))\n",
    "    else:\n",
    "        print('FRAC = 1 or 2. Now this function support 1/2 and 2/3.')\n",
    "        \n",
    "    # 2\n",
    "    #------------t = 1:T0------------#    \n",
    "    for t in range(1,T0+1):\n",
    "        if CONST_b is False:\n",
    "            # b = b(t), changing with epoch t\n",
    "            b = [-(5+4*np.sin(V*np.pi*t/(2*T)))] * H\n",
    "            theta_S = get_theta_S(a, b, m, S)\n",
    "            \n",
    "        sample={} \n",
    "        # 3\n",
    "        y_ini = y_0\n",
    "        # 4 5\n",
    "        price_hist[t] = np.random.sample(m+1) * (p_u - p_l) +  p_l\n",
    "        d_hist[t] = [np.dot(eta(price_hist[t][k-1]),theta_S[k-1]) + noise_inst[t-1][k-1] for k in range(1,m+2)]\n",
    "        \n",
    "        if t==1:\n",
    "            theta[t] = [[0]*dim] * (m+1) # set theta_1 to be zero vector\n",
    "            Lambda[t] = [[S[k]-S[k-1]] * np.identity(dim) for k in range(1,m+2)] # set Lambda_1 to be identity matrix\n",
    "            Delta_hist[t] = [gamma[k-1] * np.sqrt(np.dot(eta(price_hist[t][k-1]),inv(Lambda[t][k-1]) @ eta(price_hist[t][k-1]))) for k in range(1, m+2)]\n",
    "            det = [np.linalg.det(Lambda[t][k-1]) for k in range(1,m+2)]\n",
    "            # update Lambda[t+1] and (\\sum d,\\sum d*p)\n",
    "            Lambda[t+1] = [Lambda[t][k-1]+np.outer(eta(price_hist[t][k-1]),eta(price_hist[t][k-1])) for k in range(1,m+2)] \n",
    "            d_dtimesp = [np.array([d_hist[t][k-1],d_hist[t][k-1]*price_hist[t][k-1]]) for k in range(1, m+2)]\n",
    "            \n",
    "        else:\n",
    "            # update the detminant\n",
    "            det = [np.linalg.det(Lambda[t][k-1]) for k in range(1,m+2)]\n",
    "            theta[t] = [np.matmul(inv(Lambda[t][k-1]),d_dtimesp[k-1]) for k in range(1,m+2)]\n",
    "            \n",
    "            # for each k, compute Delta_tk(p) for p in grid, also for \\bar D_t^k\n",
    "            for k in range(1, m+2):\n",
    "                Delta = [gamma[k-1]*np.sqrt(np.dot(eta(p_grid[i]),inv(Lambda[t][k-1]) @ eta(p_grid[i]))) for i in range(p_number)]\n",
    "                UCB = [np.dot(theta[t][k-1],eta(p_grid[i])) for i in range(p_number)] # eta(p) times theta_t^k\n",
    "                D[k] = [min(UCB[i] + Delta[i], np.dot(theta_S[k-1], eta(p_l)), np.dot(theta_S[k-1],eta(p_u)) + 1*(L**2)*(p_u-p_grid[i])) for i in range(p_number)]\n",
    "                H_index = [s for s in range(1,t) if np.sqrt(t) * Delta_hist[s][k-1] < kapa]\n",
    "\n",
    "                if len(H_index) > sample_max:\n",
    "                    H_small = H_index[-sample_max:]\n",
    "                else:\n",
    "                    H_small = H_index\n",
    "                sample[k-1] = np.array([d_hist[s][k-1] - np.dot(theta[s][k-1],eta(price_hist[s][k-1])) for s in H_small])\n",
    "            \n",
    "            # update Lambda[t+1]\n",
    "            Lambda[t+1] = [Lambda[t][k-1] +np.outer(eta(price_hist[t][k-1]),eta(price_hist[t][k-1])) for k in range(1,m+2)] \n",
    "            d_dtimesp=[d_dtimesp[k-1]+np.array([d_hist[t][k-1],d_hist[t][k-1]*price_hist[t][k-1]]) for k in range(1,m+2)]\n",
    "            \n",
    "            # store the Delta of this epoch\n",
    "            Delta_hist[t]=[gamma[k-1]*np.sqrt(np.dot(eta(price_hist[t][k-1]),inv(Lambda[t][k-1]) @ eta(price_hist[t][k-1]))) for k in range(1,m+2)]\n",
    "\n",
    "        # compute the revenue\n",
    "        rev_list.append(np.dot(price_hist[t],d_hist[t])-G(proj(y_ini-sum(d_hist[t]), y_grid)))\n",
    "        if NO_OPT is False:\n",
    "            # compute the revenue and prices of the optimal policy in epoch t\n",
    "            rev_epoch_opt, price_opt_t, d_opt_t = compute_rev(m, theta_S, y_ini_optimal, pricing_optimal, noise_inst[t-1], y_grid)\n",
    "            rev_list_opt.append(rev_epoch_opt)\n",
    "\n",
    "    #------------t = T0+1------------#\n",
    "    theta[t]=[np.matmul(inv(Lambda[t][k-1]),d_dtimesp[k-1]) for k in range(1,m+2)]\n",
    "    \n",
    "    for k in range(1,m+2):\n",
    "        Delta = [gamma[k-1] * np.sqrt(np.dot(eta(p_grid[i]),inv(Lambda[t][k-1]) @ eta(p_grid[i]))) for i in range(p_number)]\n",
    "        UCB = [np.dot(theta[t][k-1],eta(p_grid[i])) for i in range(p_number)] # eta(p) times theta_t^k\n",
    "        # 6. estimate: D_t^k(p)\n",
    "        D[k] = [min(UCB[i]+Delta[i],np.dot(theta_S[k-1],eta(p_l)),np.dot(theta_S[k-1],eta(p_u))+(S[k]-S[k-1])*(L**2)*(p_u-p_grid[i])) for i in range(p_number)]\n",
    "        H_index = [s for s in range(1,t) if np.sqrt(t)*Delta_hist[s][k-1]<kapa]\n",
    "        if len(H_index) > sample_max:\n",
    "            H_small = H_index[-sample_max:]\n",
    "        else:\n",
    "            H_small = H_index\n",
    "        # 7. estimate hat{\\mu}_t^k \n",
    "        sample[k-1] = np.array([d_hist[s][k-1]-np.dot(theta[s][k-1],eta(price_hist[s][k-1])) for s in H_small])    \n",
    "\n",
    "    # 8. 9. solve DP using the estimated parameters and distributions\n",
    "    rev_to_go_estimate, p_policy_alg = solve_DP_estimate(m, D, sample, y_grid)\n",
    "    y_new = y_grid[np.asarray(rev_to_go_estimate[1]).argmax()]\n",
    "    \n",
    "    '''\n",
    "    Step 2\n",
    "    Exploit the policy has been learnt\n",
    "    '''\n",
    "\n",
    "    # 10.\n",
    "    #------------t = T0+1:T------------#\n",
    "    for t in range(T0+1,T+1):\n",
    "        if CONST_b is False:\n",
    "            # b = b(t), changing with epoch t\n",
    "            b = [-(5+4*np.sin(V*np.pi*t/(2*T)))] * H\n",
    "            theta_S = get_theta_S(a, b, m, S)\n",
    "        theta[t]=theta[t-1]\n",
    "        # compute the revenue and prices in epoch t\n",
    "        rev_epoch, price_hist[t], d_hist[t] = compute_rev(m, theta_S, y_new, p_policy_alg, noise_inst[t-1], y_grid)\n",
    "        rev_list.append(rev_epoch)\n",
    "        if NO_OPT is False:\n",
    "            # compute the revenue and prices of the optimal policy in epoch t\n",
    "            rev_epoch_opt, price_opt_t,d_opt_t = compute_rev(m, theta_S, y_ini_optimal, pricing_optimal, noise_inst[t-1], y_grid)\n",
    "            rev_list_opt.append(rev_epoch_opt)\n",
    "\n",
    "    return rev_list, rev_list_opt, y_new, theta    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a105c73-72a4-4f43-9df6-0554681ef317",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T10:51:05.693570Z",
     "start_time": "2023-06-28T10:51:05.688885Z"
    },
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def learning_regret(m, M, T, H, p_u, d_l_shift=0, sample_max=500, UNI_ALOC=False, ALG=2, FRAC=1, REGRET=True, MODIFIED=False, LOG_T=False, CONST_b = True, V_TYPE=None, TOTAL_REGRET=False, NO_OPT=False):\n",
    "    \"\"\"\n",
    "    Calculate the learning regret or profit in a pricing algorithm.\n",
    "\n",
    "    Input:\n",
    "        m (int): Number of pricing change points.\n",
    "        M (int): Repetition times.\n",
    "        T (int): Number of epochs.\n",
    "        H (int): Number of periods.\n",
    "        p_u (float): Upper price limit.\n",
    "        d_l_shift (int, optional): For Modified Alg 2(1), d_l <- d_l - d_l_shift * sigma.\n",
    "        sample_max (int, optional): Number of latest epochs for computing DP. Defaults to 500.\n",
    "        UNI_ALOC (bool, optional): Use uniform allocation if True, otherwise optimal allocation. Defaults to False.\n",
    "        ALG (int, optional): Algorithm choice, either Alg2 (2) or Exp-Exp (3). Defaults to 2.\n",
    "        FRAC (int, optional): Fraction choice for Exp-Exp Alg, either T_0 = T^(1/2) (1) or T_0 = T^(2/3) (2). Defaults to 1.\n",
    "        REGRET (bool, optional): Compute regret if True, otherwise compute profit. Defaults to True.\n",
    "        MODIFIED (bool, optional): Use modified Alg2 if True, otherwise use Alg2. Defaults to False.\n",
    "        LOG_T (bool, optional): Use log_2(T) if True, otherwise use False. Defaults to False.\n",
    "        CONST_b (bool, optional): Use True if b is const with epoch t, otherwise use False. Defaults to True.\n",
    "        V_TYPE (str, optional): Use non-const b. 'LOG', '0.2' or '0.5'. Defaults to None.\n",
    "        TOTAL_REGRET (bool, optional): Compute total regret if True, otherwise computing learning regret. Defaults to False.\n",
    "        NO_OPT (bool, optional): Compute OPT revenue if False, otherwise True. Defaults to False.\n",
    "\n",
    "    Output:\n",
    "        float: Average regret if REGRET is True, otherwise average profit.\n",
    "    \"\"\"\n",
    "    \n",
    "    noise_all={}\n",
    "    for rep_num in range(M):\n",
    "        # generate noise instances\n",
    "        noise_seq=[[np.random.normal(0,sigma[i]) for i in range(H)] for t in range(T)]\n",
    "        noise_all[rep_num]=noise_seq\n",
    "\n",
    "    # print(f'--------------T = {T}--------------')\n",
    "    # print(f'--------------m = {m}--------------')\n",
    "    # optimal allocation\n",
    "    S = get_S(m, H, UNI_ALOC)\n",
    "    \n",
    "    # aggregated parameters\n",
    "    theta_S = get_theta_S(a, b, m, S)\n",
    "    sigma_S = get_sigma_S(sigma, m, S)\n",
    "    T_2 = get_T_2(T)\n",
    "    # T_2 = np.arange(1000,4500,500)\n",
    "\n",
    "    # upper bound and lower bound of aggregated demand\n",
    "    d_l = get_d_l(p_u, theta_S, m)\n",
    "    d_u = get_d_l(p_l, theta_S, m)\n",
    "    # define the space of the inventory level, P(X>4 sigma_m)<= e^{-8}=0.00033...\n",
    "    y_u = get_y_l(d_u, H, sigma_m)\n",
    "    y_l = -y_u  \n",
    "    # y_grid=np.linspace(y_l,y_u,100)\n",
    "    y_grid = np.arange(y_l,y_u+1) # only consider integer inventory\n",
    "    # aggregated noise\n",
    "    noise_S_all = {}\n",
    "    for rep_num in range(M):\n",
    "        noise_S_all[rep_num] = get_noise_S(noise_all[rep_num], m, T, S)\n",
    "\n",
    "    if NO_OPT is False:\n",
    "        # compute the revenue to go function and the pricing policy of the optimal policy\n",
    "        rev_to_go_optimal, pricing_optimal = solve_optimal_policy(m, sigma_S, theta_S, y_grid, 500)    \n",
    "        # compute the optimal inventory level according  to u[1]\n",
    "        y_ini_optimal = y_grid[np.asarray(rev_to_go_optimal[1]).argmax()]\n",
    "    else:\n",
    "        y_ini_optimal = None\n",
    "        pricing_optimal = None\n",
    "    \n",
    "        # print('Clairvoyant optimal inventory level:', y_ini_optimal)\n",
    "        # print('Clairvoyant optimal revenue per epoch:', max(rev_to_go_optimal[1]))\n",
    "\n",
    "    if TOTAL_REGRET:\n",
    "        # define new parameters when m = H-1\n",
    "        noise_H_all = {}\n",
    "        S_H = get_S(H-1, H, UNI_ALOC)\n",
    "        theta_H = get_theta_S(a, b, H-1, S_H)\n",
    "        sigma_H = get_sigma_S(sigma, H-1, S_H)\n",
    "        rev_opt_H = [[[0] for col in range(T)] for row in range(M)]\n",
    "        d_l_h = get_d_l(p_u, theta_H, H-1)\n",
    "        d_u_h = get_d_l(p_l, theta_H, H-1)\n",
    "        y_u_h = get_y_l(d_u_h, H, sigma_m)\n",
    "        y_l_h = - y_u_h\n",
    "        y_grid_h = np.arange(y_l_h, y_u_h + 1)\n",
    "\n",
    "        # solve DP when m = H-1\n",
    "        rev_to_go_opt_H, pricing_opt_H = solve_optimal_policy(H-1, sigma_H, theta_H, y_grid_h, 500)\n",
    "        y_ini_opt_H = y_grid_h[np.asarray(rev_to_go_opt_H[1]).argmax()]\n",
    "            \n",
    "        for rep_num in range(M):\n",
    "            rev_opt_H[rep_num] = []\n",
    "            noise_H_all[rep_num] = get_noise_S(noise_all[rep_num], H-1, T, S_H)\n",
    "            rev_opt_H[rep_num] = compute_oracle(T, H-1, S, theta_H, noise_H_all[rep_num], y_ini_opt_H, pricing_opt_H, y_grid_h)\n",
    "        if LOG_T:\n",
    "            rev_H = [np.mean(mean_row(rev_opt_H)[0:t])  for t in T_2]\n",
    "            rev_H_sum = [np.sum(mean_row(rev_opt_H)[0:t])  for t in T_2]\n",
    "        else:\n",
    "            rev_H = np.mean(mean_row(rev_opt_H))\n",
    "            rev_H_sum = np.sum(mean_row(rev_opt_H))\n",
    "    \n",
    "    output={}\n",
    "    rev_alg = [[[0] for col in range(T)] for row in range(M)]\n",
    "    rev_opt = [[[0] for col in range(T)] for row in range(M)]\n",
    "    \n",
    "    for rep_num in range(M):\n",
    "        # print('------------------------------------------------------')\n",
    "        # print('start ', rep_num+1, ' round')\n",
    "        if ALG == 2:\n",
    "            output[rep_num] = learning_algorithm(m, T, H, S, theta_S, noise_S_all[rep_num], y_ini_optimal, pricing_optimal, y_u, y_grid, p_u, d_l_shift=d_l_shift, sample_max=sample_max, MODIFIED=MODIFIED, CONST_b=CONST_b, V_TYPE=V_TYPE, NO_OPT=NO_OPT)\n",
    "        elif ALG == 3:\n",
    "            output[rep_num] = Exp_Exp_algorithm(m, T, H, S, theta_S, noise_S_all[rep_num], y_grid, y_ini_optimal, pricing_optimal, d_u=d_u, sigma_S=sigma_S, sample_max=500, FRAC=FRAC, CONST_b=CONST_b, V_TYPE=V_TYPE, NO_OPT=NO_OPT)\n",
    "        else:\n",
    "            print('ALG = 2 or 3. Now this function supports the computation of Alg 2 and 3.')\n",
    "        \n",
    "        rev_alg[rep_num] = output[rep_num][0]\n",
    "        if NO_OPT is False:\n",
    "            rev_opt[rep_num] = output[rep_num][1]\n",
    "\n",
    "    if LOG_T:\n",
    "        rev_alg_sum = [np.sum(mean_row(rev_alg)[0:t])  for t in T_2]\n",
    "        rev_alg = [np.mean(mean_row(rev_alg)[0:t])  for t in T_2]\n",
    "        if NO_OPT is False:\n",
    "            rev_opt_sum = [np.sum(mean_row(rev_opt)[0:t])  for t in T_2]\n",
    "            rev_opt = [np.mean(mean_row(rev_opt)[0:t])  for t in T_2]\n",
    "    else:\n",
    "        rev_alg_sum = np.sum(mean_row(rev_alg))\n",
    "        rev_alg = np.mean(mean_row(rev_alg))\n",
    "        if NO_OPT is False:\n",
    "            rev_opt_sum = np.sum(mean_row(rev_opt))\n",
    "            rev_opt = np.mean(mean_row(rev_opt))\n",
    "    \n",
    "    if REGRET:\n",
    "        regret = minus(rev_opt_sum, rev_alg_sum)\n",
    "        if TOTAL_REGRET:\n",
    "            regret2 = minus(rev_H_sum, rev_alg_sum)\n",
    "            return regret, regret2\n",
    "        return regret\n",
    "    if NO_OPT is False:\n",
    "        return rev_alg, rev_opt\n",
    "    return rev_alg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ced7ff-8f51-4ee0-9d95-31579d570a05",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def get_first_regret(m, M, T, H, p_u, d_l_shift=0, sample_max=500, UNI_ALOC=False, ALG=2, FRAC=1, REGRET=True, MODIFIED=False, LOG_T=False, CONST_b = True, V_TYPE=None, TOTAL_REGRET=False):\n",
    "#     # m is x axis, solve 2 DP: m = H-1; and m; minus\n",
    "#     #--------------------------------------------\n",
    "#     # define new parameters when m = H-1\n",
    "#     def get_rev_opt(m):\n",
    "#         noise_H_all = {}\n",
    "#         S_H = get_S(m, H, UNI_ALOC)\n",
    "#         theta_H = get_theta_S(a, b, m, S_H)\n",
    "#         sigma_H = get_sigma_S(sigma, m, S_H)\n",
    "#         rev_opt_H = [[[0] for col in range(T)] for row in range(M)]\n",
    "#         d_l_h = get_d_l(p_u, theta_H, m)\n",
    "#         d_u_h = get_d_l(p_l, theta_H, m)\n",
    "#         y_u_h = get_y_l(d_u_h, H, sigma_m)\n",
    "#         y_l_h = - y_u_h\n",
    "#         y_grid_h = np.arange(y_l_h, y_u_h + 1)\n",
    "        \n",
    "#         # solve DP when m = H-1\n",
    "#         rev_to_go_opt_H, pricing_opt_H = solve_optimal_policy(m, sigma_H, theta_H, y_grid_h, 500)\n",
    "#         y_ini_opt_H = y_grid_h[np.asarray(rev_to_go_opt_H[1]).argmax()]\n",
    "            \n",
    "#         for rep_num in range(M):\n",
    "#             rev_opt_H[rep_num] = []\n",
    "#             noise_H_all[rep_num] = get_noise_S(noise_all[rep_num], m, T, S_H)\n",
    "#             rev_opt_H[rep_num] = compute_oracle(T, m, S, theta_H, noise_H_all[rep_num], y_ini_opt_H, pricing_opt_H, y_grid_h)\n",
    "    \n",
    "#         rev_H = np.mean(mean_row(rev_opt_H))\n",
    "#         rev_H_sum = np.sum(mean_row(rev_opt_H))\n",
    "#         return rev_H, rev_H_sum\n",
    "\n",
    "#     rev_H, rev_H_sum = get_rev_opt(H-1)\n",
    "#     rev_m, rev_m_sum = get_rev_opt(m)\n",
    "    \n",
    "#     return rev_H - rev_m\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7befc0-01a6-4191-8be6-660cb8e64715",
   "metadata": {},
   "source": [
    "# Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b059bea-dcdf-4349-a9e9-5d8a59d3a483",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# define the space of prices\n",
    "p_u=1\n",
    "p_l=0\n",
    "p_number=10\n",
    "p_grid=np.linspace(p_l,p_u,p_number)\n",
    "dim=len(eta(p_u))\n",
    "\n",
    "# holding and backlogging costs\n",
    "holding=0.05\n",
    "backlog=1\n",
    "H = 5\n",
    "a=[30]*H\n",
    "b=[-20]*H\n",
    "\n",
    "# a=[10]*H\n",
    "# b=[-5]*H\n",
    "\n",
    "# a = [20,30] * (H//2)\n",
    "# b = [-5,-20] * (H//2)\n",
    "\n",
    "# a = [20,20] * (H//2)\n",
    "# b = [-5, -5] * (H//2)\n",
    "# a.append(20)\n",
    "# b.append(-5)\n",
    "\n",
    "# a=range(20,20+H)\n",
    "# b=[-5]*H\n",
    "\n",
    "# a = [24,23,22,21,20]\n",
    "# a = [20, 21, 22, 21, 20]\n",
    "# a = [20, 25, 20, 25, 20]\n",
    "\n",
    "theta_true=[x for x in zip(a,b)] # true demand-price function is D(p) = 20-5p\n",
    "\n",
    "# noise parameter, normal\n",
    "sigma=[2]*H\n",
    "sigma_m=max(sigma)\n",
    "\n",
    "T = 2**12\n",
    "m = 3\n",
    "M = 10 # repeat M times of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e85652",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f92b6d",
   "metadata": {},
   "source": [
    "## Compare Algorithm 2 with the modified Algorithm 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7711e7-99c6-4070-aaeb-e5950ca28e72",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # compute\n",
    "# # compute Exp-exp\n",
    "# rev_alg1, rev_opt1 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=3, LOG_T=True)\n",
    "\n",
    "# # compute Alg2\n",
    "# rev_alg2, rev_opt2 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=2, LOG_T=True)\n",
    "\n",
    "# # compute Alg2 (1)\n",
    "# rev_alg3, rev_opt3 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=2, MODIFIED=1, LOG_T=True)\n",
    "# # rev_alg3_, rev_opt3_ = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=2, MODIFIED=1, LOG_T=True, d_l_shift=1)\n",
    "# # rev_alg3__, rev_opt3__ = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=2, MODIFIED=1, LOG_T=True, d_l_shift=2)\n",
    "\n",
    "# # compute Alg2 (2)\n",
    "# rev_alg4, rev_opt4 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=2, MODIFIED=2, LOG_T=True)\n",
    "\n",
    "# # store\n",
    "# with open('results/output_1_a.pkl', 'wb') as f:\n",
    "#     pickle.dump([rev_alg1, rev_opt1, rev_alg2, rev_opt2, rev_alg3, rev_opt3, rev_alg4, rev_opt4], f)\n",
    "# f.close()\n",
    "\n",
    "# # plot\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg1,label='Exp-Exp', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg3,label='Modified Alg2 (1)', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_opt1,label='Oracle solution', linestyle='--') \n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg profit')\n",
    "# plt.title('Average Profit For Modified Alg2 (1)')\n",
    "# plt.legend()\n",
    "\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg1,label='Exp-Exp', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg4,label='Modified Alg2 (2)', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_opt1,label='Oracle solution', linestyle='--') \n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg profit')\n",
    "# plt.title('Average Profit For Modified Alg2 (2)')\n",
    "# plt.legend()\n",
    "\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg2,label='Algorithm 2', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_opt1,label='Oracle solution', linestyle='--') \n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg3,label='1st alternative', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg4,label='2nd alternative', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg profit')\n",
    "# plt.title('Average profit for different UCB constructions')\n",
    "# plt.legend()\n",
    "\n",
    "# # Average profit for different UCB constructions\\\\\n",
    "# # legend: Oracle solution, Algorithm 2, 1st alternative, 2nd alternative\\\\\n",
    "\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1),rev_opt1,label='Oracle solution', linestyle='--') \n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg2,label='Algorithm 2 with cap', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg3,label='Algorithm 2', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg4,label='2nd alternative', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg profit')\n",
    "# plt.title('Average profit for different UCB constructions')\n",
    "# plt.legend()\n",
    "\n",
    "# # reload \n",
    "# with open('results/output_1_a.pkl', 'rb') as f:\n",
    "#     rev_alg1, rev_opt1, rev_alg2, rev_opt2, rev_alg3, rev_opt3, rev_alg4, rev_opt4 = pickle.load(f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b142e8-7f2c-403e-a5cc-b58c7dabadaa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # compute old\n",
    "# p_u_range = np.arange(1,4.1,0.2)\n",
    "# output_m1 = []\n",
    "# output_m2 = []\n",
    "# for i, p_u in enumerate(p_u_range):\n",
    "#         print(f'--------------- p_u = {p_u}----------------')\n",
    "#         rev_alg, rev_opt = learning_regret(m, M, T, H, p_u, REGRET=False, MODIFIED=1)\n",
    "#         output_m1.append(rev_alg)\n",
    "#         output_m2.append(rev_opt)   \n",
    "\n",
    "# output_m1_ = []\n",
    "# output_m2_ = []\n",
    "# for i, p_u in enumerate(p_u_range):\n",
    "#         print(f'--------------- p_u = {p_u}----------------')\n",
    "#         rev_alg, rev_opt = learning_regret(m, M, T, H, p_u, REGRET=False, MODIFIED=2)\n",
    "#         output_m1_.append(rev_alg)\n",
    "#         output_m2_.append(rev_opt)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23463f93-79f6-4ccb-815b-9f9fc2e3a788",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # compute new\n",
    "\n",
    "# output_m1 = []\n",
    "# output_m2 = []\n",
    "\n",
    "# output_m1_305 = []\n",
    "# output_m2_305 = []\n",
    "\n",
    "# d_l_shift_range = np.arange(0,2.2,0.2)\n",
    "# for i, d_l_shift in enumerate(d_l_shift_range):\n",
    "#         print(f'--------------- d_l_shift = {d_l_shift}----------------')\n",
    "#         rev_alg, rev_opt = learning_regret(m, M, T, H, p_u, d_l_shift=d_l_shift, REGRET=False, MODIFIED=1)\n",
    "#         output_m1_305.append(rev_alg)\n",
    "#         output_m2_305.append(rev_opt)   \n",
    "\n",
    "# output_m1_3020 = []\n",
    "# output_m2_3020 = []\n",
    "# d_l_shift_range = np.arange(0,2.2,0.2)\n",
    "# for i, d_l_shift in enumerate(d_l_shift_range):\n",
    "#         print(f'--------------- d_l_shift = {d_l_shift}----------------')\n",
    "#         rev_alg, rev_opt = learning_regret(m, M, T, H, p_u, d_l_shift=d_l_shift, REGRET=False, MODIFIED=1)\n",
    "#         output_m1_3020.append(rev_alg)\n",
    "#         output_m2_3020.append(rev_opt)   \n",
    "\n",
    "# output_m1_ = []\n",
    "# output_m2_ = []\n",
    "\n",
    "# d_l_shift_range = np.arange(0,1.2,0.2)\n",
    "# for i, d_l_shift in enumerate(d_l_shift_range):\n",
    "#         print(f'--------------- d_l_shift = {d_l_shift}----------------')\n",
    "#         rev_alg, rev_opt = learning_regret(m, M, T, H, p_u, d_l_shift=d_l_shift, REGRET=False, MODIFIED=2)\n",
    "#         output_m1_.append(rev_alg)\n",
    "#         output_m2_.append(rev_opt)    \n",
    "\n",
    "# # plot\n",
    "# # d_l_range = np.min([np.array(get_d_l(p_u, get_theta_S(a, b, m, get_S(m, H, False)), m)) - d_l_shift * sigma_m  for d_l_shift in d_l_shift_range], axis=1) \n",
    "# d_l_range = np.arange(0,2.1,0.2)\n",
    "# plt.plot(d_l_range, output_m1_305, label='1st alternative',marker='.')\n",
    "# plt.plot(d_l_range, output_m2_305, label='Oracle solution', linestyle='--')\n",
    "# plt.xlabel('multiplier')\n",
    "# plt.ylabel('avg profit')\n",
    "# plt.title(r'Average profit for different deviations ($a_i=30, b_i=-5$)')\n",
    "# plt.legend()\n",
    "\n",
    "# # plot\n",
    "# # d_l_range = np.min([np.array(get_d_l(p_u, get_theta_S(a, b, m, get_S(m, H, False)), m)) - d_l_shift * sigma_m  for d_l_shift in d_l_shift_range], axis=1) \n",
    "# d_l_range = np.arange(0,2.1,0.2)\n",
    "# plt.plot(d_l_range, output_m1_3020, label='1st alternative',marker='.')\n",
    "# plt.plot(d_l_range, output_m2_3020, label='Oracle solution', linestyle='--')\n",
    "# plt.xlabel('multiplier')\n",
    "# plt.ylabel('avg profit')\n",
    "# # plt.text(x = 1.5, y = 146, s = r'$T = 2^{10}, H data:image/png;base64,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 10, m=5, a = 20, b = -5$')\n",
    "# plt.title(r'Average profit for different deviations ($a_i=30, b_i=-20$)')\n",
    "# plt.ylim(45,56)\n",
    "# plt.legend()\n",
    "\n",
    "# # store\n",
    "# with open('results/output_1_b_new_.pkl', 'wb') as f:\n",
    "#     pickle.dump([output_m1_305, output_m2_305, output_m1_3020, output_m2_3020], f)\n",
    "# f.close()\n",
    "\n",
    "# # store\n",
    "# with open('results/output_1_b_new.pkl', 'wb') as f:\n",
    "#     pickle.dump([output_m1, output_m2, output_m1_, output_m2_], f)\n",
    "# f.close()\n",
    "\n",
    "# reload\n",
    "with open('results/output_1_b_new.pkl', 'rb') as f:\n",
    "    output_m1, output_m2, output_m1_, output_m2_ = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3063582-dc7a-4cd1-8513-e16fe8909b40",
   "metadata": {},
   "source": [
    "## Nonstationary demand functions over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ba8e1-74e1-4800-9b07-66066271d86e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # compute\n",
    "# # LOG\n",
    "# rev_alg5 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=2, LOG_T=True, CONST_b=False, V_TYPE='LOG', NO_OPT=True)\n",
    "\n",
    "# rev_alg5_1 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, CONST_b=False, V_TYPE='LOG', NO_OPT=True)\n",
    "# rev_alg5_2 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, CONST_b=False, V_TYPE='LOG', FRAC=2, NO_OPT=True)\n",
    "\n",
    "# # T^0.2 \n",
    "# rev_alg6 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=2, LOG_T=True, CONST_b=False, V_TYPE='0.2', NO_OPT=True )\n",
    "\n",
    "# rev_alg6_1 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, CONST_b=False, V_TYPE='0.2', NO_OPT=True )\n",
    "# rev_alg6_2 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, CONST_b=False, V_TYPE='0.2', FRAC=2, NO_OPT=True)\n",
    "\n",
    "# # T_0.5\n",
    "# rev_alg7 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=2, LOG_T=True, CONST_b=False, V_TYPE='0.5', NO_OPT=True )\n",
    "\n",
    "# rev_alg7_1 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, CONST_b=False, V_TYPE='0.5', NO_OPT=True )\n",
    "# rev_alg7_2 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, CONST_b=False, V_TYPE='0.5', FRAC=2, NO_OPT=True)\n",
    "\n",
    "# with open('results/output_2_TRUE_.pkl', 'rb') as f:\n",
    "#     _, rev_opt5, _, rev_opt6, _, rev_opt7 = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "# # compute LOG changing b\n",
    "# rev_alg5, rev_opt5 = learning_regret(m, 1, T, H, p_u, REGRET=False, ALG=2, LOG_T=True, CONST_b=False, V_TYPE='LOG' )\n",
    "# rev_alg5_1, rev_opt5_1 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, CONST_b=False, V_TYPE='LOG' )\n",
    "# rev_alg5_2, rev_opt5_2 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, CONST_b=False, V_TYPE='LOG', FRAC=2)\n",
    "\n",
    "# # compute T^0.2 changing b\n",
    "# rev_alg6, rev_opt6 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=2, MODIFIED=2, LOG_T=True, CONST_b=False, V_TYPE='0.2' )\n",
    "# rev_alg6_1, rev_opt6_1 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, CONST_b=False, V_TYPE='0.2' )\n",
    "# rev_alg6_2, rev_opt6_2 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, CONST_b=False, V_TYPE='0.2', FRAC=2)\n",
    "\n",
    "# # compute T_0.5 changing b\n",
    "# rev_alg7, rev_opt7 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=2, MODIFIED=2, LOG_T=True, CONST_b=False, V_TYPE='0.5' )\n",
    "# rev_alg7_1, rev_opt7_1 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, CONST_b=False, V_TYPE='0.5' )\n",
    "# rev_alg7_2, rev_opt7_2 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, CONST_b=False, V_TYPE='0.5', FRAC=2)\n",
    "\n",
    "# # plot\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg5_tmp,label=r'Algorithm 2', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_opt5,label='Oracle solution', linestyle='--')\n",
    "# # plt.plot(range(1,len(T_2)+1),rev_alg5_1,label=r'$T_0=T^{\\frac{1}{2}}$', marker='.')\n",
    "# # plt.plot(range(1,len(T_2)+1),rev_alg5_2,label=r'$T_0=T^{\\frac{2}{3}}$', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg profit')\n",
    "# plt.title(r'$V=\\log_2(T)$')\n",
    "# plt.ylim(-50, 120)\n",
    "# plt.legend(loc='lower right')\n",
    "\n",
    "# # plot\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1),rev_opt6,label='Oracle solution', linestyle='--')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg6,label=r'Algorithm 2', marker='.')\n",
    "# # plt.plot(range(1,len(T_2)+1),rev_alg6_1,label=r'$T_0=T^{\\frac{1}{2}}$', marker='.')\n",
    "# # plt.plot(range(1,len(T_2)+1),rev_alg6_2,label=r'$T_0=T^{\\frac{2}{3}}$', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg profit')\n",
    "# # plt.title(r'Nonstationary Demand Functions with $b_i(t) = -(5+sin(\\frac{V\\pi t}{2T}))$')\n",
    "# plt.title(r'$V=T^{0.2}$')\n",
    "# plt.ylim(-75, 120)\n",
    "# plt.legend(loc='lower right')\n",
    "\n",
    "# # plot\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1),rev_opt7,label='Oracle solution', linestyle='--')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg7,label='Algorithm 2', marker='.')\n",
    "# # plt.plot(range(1,len(T_2)+1),rev_alg7_1,label=r'$T_0=T^{\\frac{1}{2}}$', marker='.')\n",
    "# # plt.plot(range(1,len(T_2)+1),rev_alg7_2,label=r'$T_0=T^{\\frac{2}{3}}$', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg profit')\n",
    "# # plt.title(r'Nonstationary Demand Functions with $b_i(t) = -(5+sin(\\frac{V\\pi t}{2T}))$')\n",
    "# plt.title(r'$V=T^{0.5}$')\n",
    "# plt.ylim(-75, 120)\n",
    "# plt.legend(loc='lower right')\n",
    "\n",
    "# # store\n",
    "# with open('results/output_2_TRUE_.pkl', 'wb') as f:\n",
    "#     pickle.dump([rev_alg5, rev_opt5, rev_alg6, rev_opt6, rev_alg7, rev_opt7], f)\n",
    "# f.close()\n",
    "\n",
    "# # store\n",
    "# with open('results/output_2_TRUE_final.pkl', 'wb') as f:\n",
    "#     pickle.dump([rev_alg5, rev_alg5_1, rev_alg5_2, rev_opt5, \n",
    "#                  rev_alg6, rev_alg6_1, rev_alg6_2, rev_opt6, \n",
    "#                  rev_alg7, rev_alg7_1, rev_alg7_2, rev_opt7], f)\n",
    "# f.close()\n",
    "\n",
    "\n",
    "# # reload\n",
    "# with open('results/output_2_TRUE_.pkl', 'rb') as f:\n",
    "#     rev_alg5, rev_opt5, rev_alg6, rev_opt6, rev_alg7, rev_opt7 = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "# reload\n",
    "with open('results/output_2_TRUE_final.pkl', 'rb') as f:\n",
    "    rev_alg5, rev_alg5_1, rev_alg5_2, rev_opt5, rev_alg6, rev_alg6_1, rev_alg6_2, rev_opt6, rev_alg7, rev_alg7_1, rev_alg7_2, rev_opt7 = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b800a621-1094-4776-a96e-dbefb8cf2f1e",
   "metadata": {},
   "source": [
    "## Compare with Explore-then-exploit Heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d0d23-6db2-439e-922e-58ee6c7e3cbb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # compute\n",
    "# # compute H=5, m=3\n",
    "# # compute Exp-exp\n",
    "# rev_new_1, rev_opt_1 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=2, LOG_T=True, UNI_ALOC=False)\n",
    "# rev_new_2, rev_opt_2 = learning_regret(m, 100, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, UNI_ALOC=False)\n",
    "# rev_new_3, rev_opt_3 = learning_regret(m, 100, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, UNI_ALOC=False, FRAC=2)\n",
    "\n",
    "# # compute H=5, m=3\n",
    "# # compute Exp-exp\n",
    "# rev_new_4, rev_opt_4 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=2, LOG_T=True, UNI_ALOC=True)\n",
    "# rev_new_5, rev_opt_5 = learning_regret(m, 100, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, UNI_ALOC=True)\n",
    "# rev_new_6, rev_opt_6 = learning_regret(m, 100, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, UNI_ALOC=True, FRAC=2)\n",
    "\n",
    "# # compute H=5, m=3\n",
    "# # compute Exp-exp\n",
    "# rev_new_7 = learning_regret(m, M, T, H, p_u, REGRET=True, ALG=2, LOG_T=True, UNI_ALOC=False)\n",
    "# rev_new_8 = learning_regret(m, 100, T, H, p_u, REGRET=True, ALG=3, LOG_T=True, UNI_ALOC=False)\n",
    "# rev_new_9 = learning_regret(m, 100, T, H, p_u, REGRET=True, ALG=3, LOG_T=True, UNI_ALOC=False, FRAC=2)\n",
    "\n",
    "# # compute H=5, m=3\n",
    "# # compute Exp-exp\n",
    "# rev_new_10 = learning_regret(m, M, T, H, p_u, REGRET=True, ALG=2, LOG_T=True, UNI_ALOC=True)\n",
    "# rev_new_11 = learning_regret(m, 100, T, H, p_u, REGRET=True, ALG=3, LOG_T=True, UNI_ALOC=True)\n",
    "# rev_new_12 = learning_regret(m, 100, T, H, p_u, REGRET=True, ALG=3, LOG_T=True, UNI_ALOC=True, FRAC=2)\n",
    "\n",
    "# # plot\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1),rev_new_1,label='Algorithm 2', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_opt_4,label='Oracle solution', linestyle='--') \n",
    "# plt.plot(range(1,len(T_2)+1),rev_new_2,label=r'Exp-Exp with $T_0 = T^{\\frac{1}{2}}$', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_new_3,label=r'Exp-Exp with $T_0 = T^{\\frac{2}{3}}$', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg profit')\n",
    "# plt.title('Average profit for different time horizons (optimal allocation)')\n",
    "# plt.legend()\n",
    "\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1),rev_new_4,label='Algorithm 2', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_opt_4,label='Oracle solution', linestyle='--') \n",
    "# plt.plot(range(1,len(T_2)+1),rev_new_5,label=r'Exp-Exp with $T_0 = T^{\\frac{1}{2}}$', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_new_6,label=r'Exp-Exp with $T_0 = T^{\\frac{2}{3}}$', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg profit')\n",
    "# plt.title('Average profit for different time horizons (uniform allocation)')\n",
    "# plt.legend()\n",
    "\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1),[rev_new_7[i]/T_2[i] for i in range(len(T_2))],label='Algorithm 2', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),[rev_new_8[i]/T_2[i] for i in range(len(T_2))],label=r'Exp-Exp with $T_0 = T^{\\frac{1}{2}}$', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),[rev_new_9[i]/T_2[i] for i in range(len(T_2))],label=r'Exp-Exp with $T_0 = T^{\\frac{2}{3}}$', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg learning regret')\n",
    "# plt.title('Average learning regret for different time horizons (optimal allocation)')\n",
    "# plt.legend()\n",
    "\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1),[rev_new_10[i]/T_2[i] for i in range(len(T_2))],label='Algorithm 2', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),[rev_new_11[i]/T_2[i] for i in range(len(T_2))],label=r'Exp-Exp with $T_0 = T^{\\frac{1}{2}}$', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),[rev_new_12[i]/T_2[i] for i in range(len(T_2))],label=r'Exp-Exp with $T_0 = T^{\\frac{2}{3}}$', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg learning regret')\n",
    "# plt.title('Average learning regret for different time horizons (uniform allocation)')\n",
    "# plt.legend()\n",
    "\n",
    "# # store\n",
    "# variables = [rev_new_1, rev_opt_1, rev_new_2, rev_opt_2, rev_new_3, rev_opt_3, \n",
    "#              rev_new_4, rev_opt_4, rev_new_5, rev_opt_5, rev_new_6, rev_opt_6,\n",
    "#              rev_new_7, rev_new_8, rev_new_9, rev_new_10, rev_new_11, rev_new_12\n",
    "#             ]\n",
    "# with open('results/output_3_2.pkl', 'wb') as f:\n",
    "#     pickle.dump(variables, f)\n",
    "\n",
    "# # reload\n",
    "# with open('results/output_3_.pkl', 'rb') as f:\n",
    "#     rev_new_1, rev_opt_1, rev_new_2, rev_opt_2, rev_new_3, rev_opt_3, rev_new_4, rev_opt_4, rev_new_5, rev_opt_5, rev_new_6, rev_opt_6, rev_new_7, rev_new_8, rev_new_9, rev_new_10, rev_new_11, rev_new_12 = pickle.load(f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91951ef5-bd2a-48e4-bca3-6524bbe25f34",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# print(compute_relative_profit_loss(rev_opt_1, rev_new_1)[9:12])\n",
    "\n",
    "# print(compute_relative_profit_loss(rev_opt_2, rev_new_2)[9:12])\n",
    "\n",
    "# print(compute_relative_profit_loss(rev_opt_3, rev_new_3)[9:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426288a8-aa6a-4aa8-bbdb-c4414589c856",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # compute H=20, m=5\n",
    "# # compute Exp-exp\n",
    "# rev_alg8, rev_opt8 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=2, LOG_T=True, UNI_ALOC=True)\n",
    "\n",
    "# rev_alg9, rev_opt9 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, UNI_ALOC=True)\n",
    "\n",
    "# # compute H=20, m=5\n",
    "# # compute Exp-exp\n",
    "# rev_alg10, rev_opt10 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=2, LOG_T=True)\n",
    "\n",
    "# rev_alg11, rev_opt11 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=3, LOG_T=True)\n",
    "\n",
    "# # compute H=10, m=3 UNIFORM\n",
    "# # compute Exp-exp\n",
    "# rev_alg12, rev_opt12 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=2, LOG_T=True, UNI_ALOC=True)\n",
    "\n",
    "# rev_alg13, rev_opt13 = learning_regret(m, M, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, UNI_ALOC=True)\n",
    "\n",
    "# # plot\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg2,label='Algorithm 2', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_opt1,label='Oracle solution', linestyle='--') \n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg1,label='Exp-Exp', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg profit')\n",
    "# plt.title('Stationary demand and optimal allocation')\n",
    "# plt.legend()\n",
    "\n",
    "# # plot\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg12,label='Algorithm 2', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_opt13,label='Oracle solution', linestyle='--') \n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg13,label='Exp-Exp', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg profit')\n",
    "# plt.title('Stationary demand and uniform allocation')\n",
    "# plt.legend()\n",
    "\n",
    "# # plot\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg8,label='Algorithm 2', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_opt8,label='Oracle solution', linestyle='--') \n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg9,label='Exp-Exp', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg profit')\n",
    "# plt.title('Stationary demand and optimal allocation')\n",
    "# plt.legend()\n",
    "\n",
    "# # plot\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg10,label='Algorithm 2', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_opt10,label='Oracle solution', linestyle='--') \n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg11,label='Exp-Exp', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg profit')\n",
    "# plt.title('Stationary demand and uniform allocation')\n",
    "# plt.legend()\n",
    "\n",
    "# # store\n",
    "# with open('results/output_3.pkl', 'wb') as f:\n",
    "#     pickle.dump([rev_alg2, rev_opt2, rev_alg1, rev_opt1,\n",
    "#                 rev_alg12, rev_opt12, rev_alg13, rev_opt13,\n",
    "#                 rev_alg8, rev_opt8, rev_alg9, rev_opt9,\n",
    "#                 rev_alg10, rev_opt10, rev_alg11, rev_opt11], f)\n",
    "# f.close()\n",
    "\n",
    "# reload\n",
    "with open('results/output_3.pkl', 'rb') as f:\n",
    "    rev_alg2, rev_opt2, rev_alg1, rev_opt1, rev_alg12, rev_opt12, rev_alg13, rev_opt13, rev_alg8, rev_opt8, rev_alg9, rev_opt9, rev_alg10, rev_opt10, rev_alg11, rev_opt11 = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd406517-f974-481b-bf71-15d991c676df",
   "metadata": {},
   "source": [
    "## More figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ebfb42-76fa-4d1e-8f76-5baaaebfc2ff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # compute\n",
    "# # compute Total regret\n",
    "# reg_alg1, Treg_alg1 = learning_regret(m, 100, T, H, p_u, ALG=2, LOG_T=True, REGRET=True, TOTAL_REGRET=True)\n",
    "\n",
    "# # h=20, m=5\n",
    "# reg_alg2 = learning_regret(m, M, T, H, p_u, ALG=2, LOG_T=True, REGRET=True)\n",
    "\n",
    "# # h=20, m=5\n",
    "# rev_alg2_4, rev_opt2_4 = learning_regret(m, M, T, H, p_u, ALG=2, LOG_T=True, REGRET=False)\n",
    "\n",
    "# # plot\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1), Treg_alg1, label='total regret', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('Total regret')\n",
    "# plt.title('Total regret with optimal allocation')\n",
    "# plt.legend()\n",
    "\n",
    "# # plot\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1), reg_alg1, label='learning regret', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('Learning regret')\n",
    "# plt.title('Learning regret with optimal allocation')\n",
    "# plt.legend()\n",
    "\n",
    "# # plot\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1), [reg_alg2[i]/T_2[i] for i in range(len(T_2))], label='learning regret', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('learning regret')\n",
    "# plt.title('Average learning regret with optimal allocation')\n",
    "# plt.legend()\n",
    "\n",
    "# # plot\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1),rev_alg2_4,label='Algorithm 2', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_opt2_4,label='Oracle solution', linestyle='--') \n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg profit')\n",
    "# plt.title('Average profit for algorithm 2')\n",
    "# plt.legend()\n",
    "\n",
    "# # store\n",
    "# with open('results/output_4.pkl', 'wb') as f:\n",
    "#     pickle.dump([reg_alg1, Treg_alg1, reg_alg2, rev_alg2_4, rev_opt2_4], f)\n",
    "# f.close()\n",
    "\n",
    "# reload\n",
    "with open('results/output_4.pkl', 'rb') as f:   \n",
    "    reg_alg1, Treg_alg1, reg_alg2, rev_alg2_4, rev_opt2_4 = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c71635-ce1a-4a1c-bfd5-28a98a375c29",
   "metadata": {},
   "source": [
    "## Phase Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec6749-16e1-4729-a943-201186b8b35e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # # compute\n",
    "# m_range = np.arange(1, H)\n",
    "\n",
    "# Treg5 = [[0]]\n",
    "# reg5 = [[0]]\n",
    "# for m in m_range:\n",
    "#     reg_epoch, Treg_epoch = learning_regret(m, 1, T, H=H, p_u=p_u, ALG=2, LOG_T=True, REGRET=True, TOTAL_REGRET=True)\n",
    "#     print(reg_epoch[-1] - reg5[-1][-1])\n",
    "#     print(Treg_epoch[-1] - Treg5[-1][-1])\n",
    "#     Treg5.append(Treg_epoch)\n",
    "#     reg5.append(reg_epoch)\n",
    "\n",
    "# Treg6 = []\n",
    "# reg6 = []\n",
    "# for m in m_range:\n",
    "#     _, Treg_epoch = learning_regret(m, 1, T, H, p_u, ALG=2, REGRET=True, LOG_T=True, TOTAL_REGRET=True, UNI_ALOC=True)\n",
    "#     print(Treg_epoch)\n",
    "#     Treg6.append(Treg_epoch)\n",
    "#     reg6.append(_)\n",
    "\n",
    "# # H=10, m=5\n",
    "# m_range = np.arange(1, H)\n",
    "\n",
    "# Treg7 = []\n",
    "# for m in m_range:\n",
    "#    _, Treg_epoch = learning_regret(m, 3, T, H=H, p_u=p_u, ALG=2, LOG_T=True, REGRET=True, TOTAL_REGRET=True)\n",
    "#    print(Treg_epoch)\n",
    "#    Treg7.append(Treg_epoch)\n",
    "\n",
    "# Treg8 = []\n",
    "# for m in m_range:\n",
    "#    _, Treg_epoch = learning_regret(m, 3, T, H, p_u, ALG=2, REGRET=True, LOG_T=True, TOTAL_REGRET=True, UNI_ALOC=True)\n",
    "#    print(Treg_epoch)\n",
    "#    Treg8.append(Treg_epoch)\n",
    "\n",
    "# # plot\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(m_range, [reg6[i][-1] for i in range(len(reg6))], label=r'uniform allocation', marker='.')\n",
    "# plt.plot(m_range, [reg5[i][-1] for i in range(len(reg5))], label=r'optimal allocation', marker='.')\n",
    "# plt.xlabel(r'$m$')\n",
    "# plt.ylabel('learning regret')\n",
    "# plt.title(r'Learning regret of different allocations ($T=2^{12}$)')\n",
    "# plt.legend()\n",
    "\n",
    "# # plot\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(m_range, [Treg6[i][-1] for i in range(len(Treg6))], label=r'uniform allocation', marker='.')\n",
    "# plt.plot(m_range, [Treg5[i][-1] for i in range(len(Treg5))], label=r'optimal allocation', marker='.')\n",
    "# plt.xlabel(r'$m$')\n",
    "# plt.ylabel('regret')\n",
    "# plt.title('Regret of different allocations')\n",
    "# plt.legend()\n",
    "# # plt.ylim(8000,30000)\n",
    "\n",
    "# # plot\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(m_range, [Treg6[i][9] for i in range(len(Treg6))], label=r'$T=2^{10}$', marker='.')\n",
    "# plt.plot(m_range, [Treg6[i][10] for i in range(len(Treg6))], label=r'$T=2^{11}$', marker='.')\n",
    "# plt.plot(m_range, [Treg6[i][11] for i in range(len(Treg6))], label=r'$T=2^{12}$', marker='.')\n",
    "# plt.xlabel(r'$m$')\n",
    "# plt.ylabel('Total regret')\n",
    "# plt.title('Total regret with uniform allocation')\n",
    "# plt.legend()\n",
    "\n",
    "# # plot\n",
    "# T_2 = get_T_2(T)\n",
    "# # plt.plot(m_range, [Treg7[i][9] for i in range(len(Treg7))], label=r'$T=2^{10}$', marker='.')\n",
    "# # plt.plot(m_range, [Treg7[i][10] for i in range(len(Treg7))], label=r'$T=2^{11}$', marker='.')\n",
    "# plt.plot(m_range, [Treg7[i][11] for i in range(len(Treg7))], label=r'$T=2^{12}$', marker='.')\n",
    "# plt.xlabel(r'$m$')\n",
    "# plt.ylabel('Total regret')\n",
    "# plt.title('Total regret with optimal allocation')\n",
    "# plt.legend()\n",
    "\n",
    "# # plot\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(m_range, [Treg8[i][9] for i in range(len(Treg8))], label=r'$T=2^{10}$', marker='.')\n",
    "# plt.plot(m_range, [Treg8[i][10] for i in range(len(Treg8))], label=r'$T=2^{11}$', marker='.')\n",
    "# plt.plot(m_range, [Treg8[i][11] for i in range(len(Treg8))], label=r'$T=2^{12}$', marker='.')\n",
    "# plt.xlabel(r'$m$')\n",
    "# plt.ylabel('Total regret')\n",
    "# plt.title('Total regret with uniform allocation')\n",
    "# plt.legend()\n",
    "\n",
    "# # store\n",
    "# with open('results/output_5_tmp.pkl', 'wb') as f:\n",
    "#     pickle.dump([Treg5, Treg6, reg5, reg6], f)\n",
    "# f.close()\n",
    "\n",
    "# reload\n",
    "with open('results/output_5_.pkl', 'rb') as f:   \n",
    "    Treg5, Treg6, Treg7, Treg8 = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69050d8-ff51-4ffb-a051-dfd23a642884",
   "metadata": {},
   "source": [
    "## Compare uniform allocation and optimal allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ed326-048a-4237-beed-153f59dd8227",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # compute\n",
    "# # H=10\n",
    "# output_6 = []\n",
    "# for m in np.arange(1,H):\n",
    "#     new_reg = learning_regret(m, 1, T, H=H, p_u=p_u, ALG=2, MODIFIED=2, REGRET=True)\n",
    "#     print(new_reg)\n",
    "#     output_6.append(new_reg)\n",
    "\n",
    "# # H=10\n",
    "# output_7 = []\n",
    "# for m in np.arange(1,H):\n",
    "#     new_reg = learning_regret(m, 1, T, H=H, p_u=p_u, ALG=2, MODIFIED=2, REGRET=True, UNI_ALOC=True)\n",
    "#     print(new_reg)\n",
    "#     output_7.append(new_reg)\n",
    "\n",
    "# plt.plot(range(1,len(output_6)+1), output_6, label='optimal allocation',marker='.')\n",
    "# # plt.plot(range(1,len(output_7)+1), output_7, label='uniform alllocation',marker='.')\n",
    "# plt.xlabel('m')\n",
    "# plt.ylabel('Learning Regret')\n",
    "# # plt.text(x = 7, y = 1500, s = r'$T = 2^{7}, H = 15, a = 20, b = -5$')\n",
    "# plt.title('Learning Regret of Different Allocation')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.plot(range(1,len(output_6)+1), output_6, label='optimal allocation',marker='.')\n",
    "# plt.plot(range(1,len(output_7)+1), output_7, label='uniform alllocation',marker='.')\n",
    "# plt.xlabel('m')\n",
    "# plt.ylabel('Learning Regret')\n",
    "# # plt.text(x = 7, y = 1500, s = r'$T = 2^{7}, H = 15, a = 20, b = -5$')\n",
    "# plt.title('Learning Regret of Different Allocation')\n",
    "# plt.legend()\n",
    "\n",
    "# # store\n",
    "# with open('results/output_6_tmp.pkl', 'wb') as f:\n",
    "#     pickle.dump([output_6, output_7], f)\n",
    "# f.close()\n",
    "\n",
    "# reload\n",
    "with open('results/output_6_tmp.pkl', 'rb') as f:   \n",
    "    output_6, output_7 = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc28a43-d10a-4372-ac44-6fe4fa4b7634",
   "metadata": {},
   "source": [
    "## Compare average total regret for m = H − 3, H − 2, H − 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a236242a-156a-4038-a11f-c4b68e3bf187",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # compute\n",
    "# # H=10\n",
    "# output_8 = []\n",
    "# output_9 = []\n",
    "\n",
    "# T_2 = get_T_2(T)\n",
    "\n",
    "# for m in np.arange(H-3,H):\n",
    "#     new_reg, Tnew_reg = learning_regret(m, 3, T, H=H, p_u=p_u, ALG=2, REGRET=True, LOG_T=True, TOTAL_REGRET=True)\n",
    "#     new_reg = [new_reg[i] / T_2[i] for i in range(len(T_2))]\n",
    "#     Tnew_reg = [Tnew_reg[i] / T_2[i] for i in range(len(T_2))]\n",
    "#     print(f'm = {m} has finished...')\n",
    "#     output_8.append(new_reg)\n",
    "#     output_9.append(Tnew_reg)\n",
    "\n",
    "# output_8_ = []\n",
    "# output_9_ = []\n",
    "# for m in np.arange(H-3,H):\n",
    "#     new_reg, Tnew_reg = learning_regret(m, 3, T, H=H, p_u=p_u, ALG=2, REGRET=True, LOG_T=True, TOTAL_REGRET=True, UNI_ALOC=True)\n",
    "#     new_reg = [new_reg[i] / T_2[i] for i in range(len(T_2))]\n",
    "#     Tnew_reg = [Tnew_reg[i] / T_2[i] for i in range(len(T_2))]\n",
    "#     print(f'm = {m} has finished...')\n",
    "#     output_8_.append(new_reg)\n",
    "#     output_9_.append(Tnew_reg)\n",
    "\n",
    "# # plot\n",
    "# plt.plot(range(10,len(output_9[0])+1), output_9[0][9:], label='m=H-3',marker='.')\n",
    "# plt.plot(range(10,len(output_9[1])+1), output_9[1][9:], label='m=H-2',marker='.')\n",
    "# plt.plot(range(10,len(output_9[2])+1), output_9[2][9:], label='m=H-1',marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg regret')\n",
    "# plt.title('Average regret for different pricing flexibilities')\n",
    "# plt.legend()\n",
    "\n",
    "# plot\n",
    "plt.plot(range(1,len(output_9[0])+1), minus(output_9_[0],output_9[0]), label='m=H-3',marker='.')\n",
    "plt.plot(range(1,len(output_9[1])+1), minus(output_9_[1],output_9[1]), label='m=H-2',marker='.')\n",
    "plt.plot(range(1,len(output_9[2])+1), minus(output_9_[2],output_9[2]), label='m=H-1',marker='.')\n",
    "plt.xlabel(r'$log_2(T)$')\n",
    "plt.ylabel('Average Total Regret')\n",
    "plt.title('Average total regret of optimal allocation')\n",
    "plt.legend()\n",
    "\n",
    "# m is x axis, solve 2 DP: m = H-1; and m; minus\n",
    "# first term\n",
    "\n",
    "plt.plot(range(10,len(output_9_[0])+1), output_9_[0][9:], label='m=H-3',marker='.')\n",
    "plt.plot(range(10,len(output_9_[1])+1), output_9_[1][9:], label='m=H-2',marker='.')\n",
    "plt.plot(range(10,len(output_9_[2])+1), output_9_[2][9:], label='m=H-1',marker='.')\n",
    "plt.xlabel(r'$log_2(T)$')\n",
    "plt.ylabel('Average Total Regret')\n",
    "plt.title('Average total regret of uniform allocation')\n",
    "plt.legend()\n",
    "\n",
    "# # store\n",
    "# with open('results/output_7_tmp.pkl', 'wb') as f:\n",
    "#     pickle.dump([output_8, output_9, output_8_, output_9_], f)\n",
    "# f.close()\n",
    "\n",
    "# reload\n",
    "with open('results/output_7_tmp.pkl', 'rb') as f:   \n",
    "    output_8, output_9, output_8_, output_9_ = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf13d65-0280-4b50-b12f-860d700c7027",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # compute\n",
    "# # H=10\n",
    "# output_8 = []\n",
    "# output_9 = []\n",
    "\n",
    "# T_2 = get_T_2(T)\n",
    "\n",
    "# for m in np.arange(H-3,H):\n",
    "#     new_reg, Tnew_reg = learning_regret(m, 3, T, H=H, p_u=p_u, ALG=2, REGRET=True, LOG_T=True, TOTAL_REGRET=True)\n",
    "#     new_reg = [new_reg[i] / T_2[i] for i in range(len(T_2))]\n",
    "#     Tnew_reg = [Tnew_reg[i] / T_2[i] for i in range(len(T_2))]\n",
    "#     print(f'm = {m} has finished...')\n",
    "#     output_8.append(new_reg)\n",
    "#     output_9.append(Tnew_reg)\n",
    "\n",
    "# output_8_ = []\n",
    "# output_9_ = []\n",
    "# for m in np.arange(H-3,H):\n",
    "#     new_reg, Tnew_reg = learning_regret(m, 3, T, H=H, p_u=p_u, ALG=2, REGRET=True, LOG_T=True, TOTAL_REGRET=True, UNI_ALOC=True)\n",
    "#     new_reg = [new_reg[i] / T_2[i] for i in range(len(T_2))]\n",
    "#     Tnew_reg = [Tnew_reg[i] / T_2[i] for i in range(len(T_2))]\n",
    "#     print(f'm = {m} has finished...')\n",
    "#     output_8_.append(new_reg)\n",
    "#     output_9_.append(Tnew_reg)\n",
    "\n",
    "# # plot\n",
    "# plt.plot(range(10,len(output_9[0])+1), output_9[0][9:], label='m=H-3',marker='.')\n",
    "# plt.plot(range(10,len(output_9[1])+1), output_9[1][9:], label='m=H-2',marker='.')\n",
    "# plt.plot(range(10,len(output_9[2])+1), output_9[2][9:], label='m=H-1',marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('Average Total Regret')\n",
    "# plt.title('Average total regret of optimal allocation')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.plot(range(10,len(output_9_[0])+1), output_9_[0][9:], label='m=H-3',marker='.')\n",
    "# plt.plot(range(10,len(output_9_[1])+1), output_9_[1][9:], label='m=H-2',marker='.')\n",
    "# plt.plot(range(10,len(output_9_[2])+1), output_9_[2][9:], label='m=H-1',marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('Average Total Regret')\n",
    "# plt.title('Average total regret of uniform allocation')\n",
    "# plt.legend()\n",
    "\n",
    "# T_2 = get_T_2(T)\n",
    "# # plot\n",
    "# plt.plot([2**i for i in range(1,len(output_9[0])+1)], [output_9[0][i] * T_2[i] for i in range(len(T_2))], label='m=H-3',marker='.')\n",
    "# plt.plot([2**i for i in range(1,len(output_9[1])+1)], [output_9[1][i] * T_2[i] for i in range(len(T_2))], label='m=H-2',marker='.')\n",
    "# plt.plot([2**i for i in range(1,len(output_9[2])+1)], [output_9[2][i] * T_2[i] for i in range(len(T_2))], label='m=H-1',marker='.')\n",
    "# plt.xlabel(r'$T$')\n",
    "# plt.ylabel('regret')\n",
    "# plt.title('Regret for different pricing flexibilities')\n",
    "# plt.legend()\n",
    "\n",
    "# # store\n",
    "# with open('results/output_7_tmp.pkl', 'wb') as f:\n",
    "#     pickle.dump([output_8, output_9, output_8_, output_9_], f)\n",
    "# f.close()\n",
    "\n",
    "\n",
    "# reload\n",
    "with open('results/output_7_tmp.pkl', 'rb') as f:   \n",
    "    output_8, output_9, output_8_, output_9_ = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c3939-44fe-4664-901d-726a2ddf84c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # compute\n",
    "# # H=10\n",
    "# output_8 = []\n",
    "# output_9 = []\n",
    "\n",
    "# T_2 = np.arange(1000,4500,500)\n",
    "# T_2.tolist()\n",
    "\n",
    "# for m in np.arange(H-3,H):\n",
    "#     new_reg, Tnew_reg = learning_regret(m, 3, T, H=H, p_u=p_u, ALG=2, REGRET=True, LOG_T=True, TOTAL_REGRET=True)\n",
    "#     new_reg = [new_reg[i] / T_2[i] for i in range(len(T_2))]\n",
    "#     Tnew_reg = [Tnew_reg[i] / T_2[i] for i in range(len(T_2))]\n",
    "#     print(f'm = {m} has finished...')\n",
    "#     output_8.append(new_reg)\n",
    "#     output_9.append(Tnew_reg)\n",
    "\n",
    "# # plot\n",
    "# plt.plot(T_2, [output_9[0][i] * T_2[i] for i in range(len(T_2))], label='m=H-3',marker='.')\n",
    "# plt.plot(T_2, [output_9[1][i] * T_2[i] for i in range(len(T_2))], label='m=H-2',marker='.')\n",
    "# plt.plot(T_2, [output_9[2][i] * T_2[i] for i in range(len(T_2))], label='m=H-1',marker='.')\n",
    "# plt.xlabel(r'$T$')\n",
    "# plt.ylabel('regret')\n",
    "# plt.title('Regret for different pricing flexibilities')\n",
    "# plt.legend()\n",
    "\n",
    "# # store\n",
    "# with open('results/output_7_1000_4000.pkl', 'wb') as f:\n",
    "#     pickle.dump([output_8, output_9], f)\n",
    "# f.close()\n",
    "\n",
    "# reload\n",
    "with open('results/output_7_1000_4000.pkl', 'rb') as f:   \n",
    "    output_8, output_9 = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a3029-74f0-455a-a321-db686eaf0990",
   "metadata": {},
   "source": [
    "## Changing Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c421c8d-caac-4427-8086-18b8e563368c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # compute\n",
    "# # compute H=5, m=3\n",
    "# # compute increasing\n",
    "# rev_ch_1, rev_cop_1 = learning_regret(m, 100, T, H, p_u, REGRET=False, ALG=2, LOG_T=True, UNI_ALOC=False)\n",
    "# rev_ch_2, rev_cop_2 = learning_regret(m, 100, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, UNI_ALOC=False)\n",
    "# rev_ch_3, rev_cop_3 = learning_regret(m, 100, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, UNI_ALOC=False, FRAC=2)\n",
    "\n",
    "\n",
    "# # compute decreasing\n",
    "# rev_ch_4, rev_cop_4 = learning_regret(m, 100, T, H, p_u, REGRET=False, ALG=2, LOG_T=True, UNI_ALOC=False)\n",
    "# rev_ch_5, rev_cop_5 = learning_regret(m, 100, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, UNI_ALOC=False)\n",
    "# rev_ch_6, rev_cop_6 = learning_regret(m, 100, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, UNI_ALOC=False, FRAC=2)\n",
    "\n",
    "\n",
    "# # compute increasing then decreasing\n",
    "# rev_ch_7, rev_cop_7 = learning_regret(m, 100, T, H, p_u, REGRET=False, ALG=2, LOG_T=True, UNI_ALOC=False)\n",
    "# rev_ch_8, rev_cop_8 = learning_regret(m, 100, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, UNI_ALOC=False)\n",
    "# rev_ch_9, rev_cop_9 = learning_regret(m, 100, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, UNI_ALOC=False, FRAC=2)\n",
    "\n",
    "\n",
    "# # compute non-monotone\n",
    "# rev_ch_10, rev_cop_10 = learning_regret(m, 100, T, H, p_u, REGRET=False, ALG=2, LOG_T=True, UNI_ALOC=False)\n",
    "# rev_ch_11, rev_cop_11 = learning_regret(m, 100, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, UNI_ALOC=False)\n",
    "# rev_ch_12, rev_cop_12 = learning_regret(m, 100, T, H, p_u, REGRET=False, ALG=3, LOG_T=True, UNI_ALOC=False, FRAC=2)\n",
    "\n",
    "# # plot\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1),rev_ch_1,label='Algorithm 2', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_cop_1,label='Oracle solution', linestyle='--') \n",
    "# plt.plot(range(1,len(T_2)+1),rev_ch_2,label=r'Exp-Exp with $T_0 = T^{\\frac{1}{2}}$', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_ch_3,label=r'Exp-Exp with $T_0 = T^{\\frac{2}{3}}$', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg profit')\n",
    "# plt.title('Increasing')\n",
    "# plt.legend()\n",
    "\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1),rev_ch_4,label='Algorithm 2', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_cop_4,label='Oracle solution', linestyle='--') \n",
    "# plt.plot(range(1,len(T_2)+1),rev_ch_5,label=r'Exp-Exp with $T_0 = T^{\\frac{1}{2}}$', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_ch_6,label=r'Exp-Exp with $T_0 = T^{\\frac{2}{3}}$', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg profit')\n",
    "# plt.title('Decreasing')\n",
    "# plt.legend()\n",
    "\n",
    "# T_2 = get_T_2(T)\n",
    "# plt.plot(range(1,len(T_2)+1),rev_ch_7,label='Algorithm 2', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_cop_7,label='Oracle solution', linestyle='--') \n",
    "# plt.plot(range(1,len(T_2)+1),rev_ch_8,label=r'Exp-Exp with $T_0 = T^{\\frac{1}{2}}$', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_ch_9,label=r'Exp-Exp with $T_0 = T^{\\frac{2}{3}}$', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg profit')\n",
    "# plt.title('Increasing then decreasing')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.plot(range(1,len(T_2)+1),rev_ch_10,label='Algorithm 2', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_cop_10,label='Oracle solution', linestyle='--') \n",
    "# plt.plot(range(1,len(T_2)+1),rev_ch_11,label=r'Exp-Exp with $T_0 = T^{\\frac{1}{2}}$', marker='.')\n",
    "# plt.plot(range(1,len(T_2)+1),rev_ch_12,label=r'Exp-Exp with $T_0 = T^{\\frac{2}{3}}$', marker='.')\n",
    "# plt.xlabel(r'$log_2(T)$')\n",
    "# plt.ylabel('avg profit')\n",
    "# plt.title('Nonmonotone')\n",
    "# plt.legend()\n",
    "\n",
    "# # store\n",
    "# with open('results/output_change_demand.pkl', 'wb') as f:\n",
    "#     pickle.dump([rev_ch_1, rev_cop_1, rev_ch_2, rev_cop_2, rev_ch_3, rev_cop_3, rev_ch_4, rev_cop_4, rev_ch_5, rev_cop_5,rev_ch_6, rev_cop_6\n",
    "#                 ,rev_ch_7, rev_cop_7, rev_ch_8, rev_cop_8, rev_ch_9, rev_cop_9, rev_ch_10, rev_cop_10, rev_ch_11, rev_cop_11, rev_ch_12, rev_cop_12], f)\n",
    "# f.close()\n",
    "\n",
    "# reload\n",
    "with open('results/output_change_demand.pkl', 'rb') as f:   \n",
    "    rev_ch_1, rev_cop_1, rev_ch_2, rev_cop_2, rev_ch_3, rev_cop_3, rev_ch_4, rev_cop_4, rev_ch_5, rev_cop_5,rev_ch_6, rev_cop_6,rev_ch_7, rev_cop_7, rev_ch_8, rev_cop_8, rev_ch_9, rev_cop_9, rev_ch_10, rev_cop_10, rev_ch_11, rev_cop_11, rev_ch_12, rev_cop_12 = pickle.load(f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
